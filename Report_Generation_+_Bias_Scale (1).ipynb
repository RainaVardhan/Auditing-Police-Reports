{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbDguQ24x8S2"
      },
      "source": [
        "# Creating and Downloading Reports in Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxnxSlq4yDw4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        " #if needed, uncomment and install\n",
        " #!pip install --quiet --upgrade openai python-dotenv pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RainaVardhan/Auditing-Police-Reports.git"
      ],
      "metadata": {
        "id": "0u70NFcHx9Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XfhOqhetepl"
      },
      "outputs": [],
      "source": [
        "#setup\n",
        "import os, getpass, sys, time\n",
        "import re, glob\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from textblob import TextBlob\n",
        "\n",
        "load_dotenv()  #loads .env if present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK2HFYLpySUJ"
      },
      "outputs": [],
      "source": [
        "def get_openai_key():\n",
        "    key = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
        "    if key:\n",
        "      return key\n",
        "\n",
        "    print(\"OpenAI key not found in system. Paste it once: (input hidden)\")\n",
        "    key = getpass.getpass(\"OpenAI API Key: \").strip()\n",
        "    if not key:\n",
        "      raise ValueError(\"No OpenAI key provided\")\n",
        "\n",
        "    #keeping in memory for this session\n",
        "    os.environ[\"OPENAI_API_KEY\"] = key\n",
        "    return key\n",
        "\n",
        "#defining open_ai_key variable\n",
        "OPEN_AI_KEY = get_openai_key()\n",
        "print(\"Key is set up!\")\n",
        "\n",
        "client = OpenAI(api_key=OPEN_AI_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g05mi1mpphCw"
      },
      "outputs": [],
      "source": [
        "#importing our \"Transcripts (Data)\" file and configuring our variables\n",
        "REPO_ROOT = \"/content/Auditing-Police-Reports\"\n",
        "BATCH_DIR = os.path.join(REPO_ROOT, \"Report Batches\")\n",
        "RESULTS_DIR = os.path.join(REPO_ROOT, \"Results\")\n",
        "\n",
        "TRANSCRIPT_COL = \"Transcript\" ##matching the column name from \"Transcripts (Data).xlsx\"\n",
        "EXCEL_PATH = f\"{REPO_ROOT}/Transcripts (Data).xlsx\"\n",
        "\n",
        "BATCH_SIZE = 25 #process 25 rows per batch\n",
        "MODEL_NAME = \"gpt-4.1-mini\" #fast, reliable model for generation\n",
        "TEMP = 0.2 #choosing a lower temp for reproducible outputs\n",
        "MAX_TOKENS = 1200 #about 600-900 words\n",
        "PAUSE_SECONDS = 2.0 #Gentle pacing between API calls\n",
        "OUTPUT_PREFIX = \"incident_reports_batch_\"\n",
        "\n",
        "os.makedirs(BATCH_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Batch dir:\", BATCH_DIR)\n",
        "print(\"Results dir:\", RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4dwmFJohC-"
      },
      "source": [
        "* Changed the model from gpt-5 to gpt-4.1-mini because it is faster, lower cost, and overall better for batch work.\n",
        "\n",
        "* Changing the temperature to 0.2 . A lower temperature lowers randomness to create more deterministic phrasing. We do want the model to be \"creative\" but not random; we should have a consistent base model for comparing biases across all tiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYC3ZuQw1dW0"
      },
      "outputs": [],
      "source": [
        "#turning our transcript data into a data frame\n",
        "try:\n",
        "    df = pd.read_excel(EXCEL_PATH)\n",
        "    print(f\"Loaded {len(df)} rows from:\", EXCEL_PATH)\n",
        "    print(\"Columns:\", list(df.columns))\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Could not read Excel at {EXCEL_PATH}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3u6W9aWudxS"
      },
      "outputs": [],
      "source": [
        "#in this system prompt, we tell the model NOT to include facts that aren't in the narrative\n",
        "SYSTEM_PROMPT = f\"\"\"\n",
        "    You are a police officer drafting an official incident report.\n",
        "\n",
        "    Write a complete incident report of approximately 350–500 words in narrative form\n",
        "    based on the events and audio described in the following transcript.\n",
        "    \"\"\"\n",
        "\n",
        "def _chat_completion(messages, model=MODEL_NAME, temperature=TEMP, max_tokens=MAX_TOKENS):\n",
        "    #call chat completions\n",
        "    attempt, delay = 0, 2.0\n",
        "    while True:\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                messages=messages,\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            attempt += 1\n",
        "            if attempt > 5:\n",
        "                #Give a readable error string so the pipeline continues\n",
        "                return f\"[ERROR after {attempt} attempts: {e}]\"\n",
        "            time.sleep(delay)\n",
        "            delay = min(delay * 1.8, 20.0)  # exponential backoff up to 20s\n",
        "\n",
        "def generate_one_report(narrative: str, case_id: int) -> str:\n",
        "  #this generates a single incident report for a given transcript\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "      {\"role\": \"user\", \"content\": (\n",
        "          f\"Write a complete incident report (~350-500) in 1st person narrative form based on the events and audio described in the following transcript.\\n\"\n",
        "          f\"Begin the report with a line 'CASE_ID: {case_id}'.\\n\\n\"\n",
        "          f\"Narrative:\\n{narrative}\"\n",
        "      )},\n",
        "  ]\n",
        "  return _chat_completion(messages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7b1Sp-bvRZ4"
      },
      "outputs": [],
      "source": [
        "#generating batches (default is 25 rows of prompts for one batch)\n",
        "def generate_incident_reports_batch(start_row: int,\n",
        "                                    batch_size: int = BATCH_SIZE,\n",
        "                                    pause_seconds: float = PAUSE_SECONDS,\n",
        "                                    output_prefix: str = OUTPUT_PREFIX):\n",
        "    n = len(df)\n",
        "    if start_row < 0 or start_row >= n:\n",
        "        raise IndexError(f\"start_row must be in [0, {n-1}], got {start_row}\")\n",
        "    end_row = min(start_row + batch_size, n)\n",
        "    batch_rows = range(start_row, end_row)\n",
        "\n",
        "    print(f\"Generating reports for rows {start_row}-{end_row-1} \"\n",
        "          f\"({end_row - start_row} total) — {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    outputs = []\n",
        "    for idx in batch_rows:\n",
        "        case_id = idx + 1\n",
        "        transcript_text = \"\" if pd.isna(df.loc[idx, TRANSCRIPT_COL]) else str(df.loc[idx, TRANSCRIPT_COL]).strip()\n",
        "\n",
        "        #skip if df already has a previous Incident_Report column with content\n",
        "        existing = df.loc[idx, \"Incident_Report\"] if \"Incident_Report\" in df.columns else None\n",
        "        if existing and isinstance(existing, str) and existing.strip() and not existing.startswith(\"[ERROR\"):\n",
        "            report_text = existing.strip()\n",
        "            print(f\"Row {idx} (CASE_ID {case_id}) already has a report — skipping.\")\n",
        "        else:\n",
        "            if not transcript_text:\n",
        "                report_text = \"[EMPTY transcript row]\"\n",
        "                print(f\"Row {idx} (CASE_ID {case_id}) has empty transcript.\")\n",
        "            else:\n",
        "                report_text = generate_one_report(transcript_text, case_id)\n",
        "                if report_text.startswith(\"[ERROR\"):\n",
        "                    print(f\"Error on row {idx} (CASE_ID {case_id}).\")\n",
        "                else:\n",
        "                    print(f\"Row {idx} (CASE_ID {case_id}) done.\")\n",
        "\n",
        "        outputs.append({\n",
        "            \"CASE_ID\": case_id,\n",
        "            \"Tier\": df.loc[idx, \"Tier\"],\n",
        "            TRANSCRIPT_COL: transcript_text,\n",
        "            \"Incident_Report\": report_text\n",
        "        })\n",
        "\n",
        "        time.sleep(pause_seconds)\n",
        "\n",
        "    batch_num = start_row // batch_size + 1\n",
        "    out_name = f\"{output_prefix}{batch_num}.xlsx\"\n",
        "    out_path = os.path.join(BATCH_DIR, out_name)\n",
        "\n",
        "    pd.DataFrame(outputs).to_excel(out_path, index=False)\n",
        "    print(f\"Saved batch to {out_path}\")\n",
        "\n",
        "    #this triggers a download in colab (optional)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(out_path)\n",
        "        print(\"Download started.\")\n",
        "    except Exception:\n",
        "        pass  #ignores if not in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_vY2Hsq5M1e"
      },
      "outputs": [],
      "source": [
        "#generating first 25 rows\n",
        "generate_incident_reports_batch(0) #start row is 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: There are 149 rows in the Transcript Data excel."
      ],
      "metadata": {
        "id": "9KQT6szc6Kbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_incident_reports_batch(140)"
      ],
      "metadata": {
        "id": "_68A_NIE50W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ-WZc4G5ZEX"
      },
      "outputs": [],
      "source": [
        "#preview output so we dont have to download every time\n",
        "def preview_outputs(path_glob=os.path.join(BATCH_DIR, f\"{OUTPUT_PREFIX}*.xlsx\"), max_reports=1): #max only one report for now\n",
        "    files = sorted(glob.glob(path_glob))\n",
        "    if not files:\n",
        "        print(\"No batch files found yet. Run a batch first.\")\n",
        "        return\n",
        "    latest = files[-1]\n",
        "    print(\"Previewing file:\", latest)\n",
        "    df_out = pd.read_excel(latest)\n",
        "    for i, row in df_out.head(max_reports).iterrows():\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"CASE_ID: {row.get('CASE_ID')}\")\n",
        "        print(\"-\"*80)\n",
        "        print(row.get(\"Incident_Report\", \"\"))\n",
        "\n",
        "preview_outputs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB-xwRRp_KWN"
      },
      "source": [
        "# Bias Scale (Attempt 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnlwgT2N_ccc"
      },
      "outputs": [],
      "source": [
        "#three tier bias scale\n",
        "\n",
        "#configuring batch into glob\n",
        "BATCH_GLOB = os.path.join(BATCH_DIR, f\"{OUTPUT_PREFIX}*.xlsx\")\n",
        "\n",
        "BATCH_TIER_MAP = {\n",
        "    1: \"Tier1_Neutral\",\n",
        "    2: \"Tier2_Subjective\",\n",
        "    3: \"Tier3_LegalUndermining\",\n",
        "}\n",
        "\n",
        "COL_CASE = \"CASE_ID\"\n",
        "COL_TEXT = \"Incident_Report\"\n",
        "COL_TRANSCRIPT = \"Transcript\"\n",
        "\n",
        "def load_batches(pattern=BATCH_GLOB):\n",
        "    # load all batched files\n",
        "    files = sorted(glob.glob(pattern))\n",
        "    if not files:\n",
        "        raise RuntimeError(f\"No files found matching {pattern}. Run generation first.\")\n",
        "    frames = []\n",
        "    for path in files:\n",
        "        m = re.search(r\"batch_(\\d+)\\.xlsx$\", path)\n",
        "        batch_num = int(m.group(1)) if m else None\n",
        "        dfb = pd.read_excel(path)\n",
        "        dfb[\"_batch_num\"] = batch_num\n",
        "        frames.append(dfb)\n",
        "    out = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    if \"Tier\" not in out.columns:\n",
        "        out[\"Tier\"] = out[\"_batch_num\"].map(BATCH_TIER_MAP).fillna(\"Unknown\")\n",
        "    return out\n",
        "\n",
        "reports = load_batches()\n",
        "print(\"Loaded rows:\", len(reports))\n",
        "print(\"Columns:\", list(reports.columns))\n",
        "\n",
        "reports.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4NIlz2T7OYN"
      },
      "source": [
        "Code above sets three defined tiers as described in the \"Linguistic Research\" Google Doc. We start by defining column names: Case_ID and Incident_Reports defined as COL_CASE and COL_TEXT, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dag18OK4mSP1"
      },
      "outputs": [],
      "source": [
        "#limited list of words, would need to expand\n",
        "# Tier 1 — Neutral/Objective indicators\n",
        "  #direct quotes\n",
        "  #time/place details\n",
        "  #action statements (e.g., \"subject walked away\", factual \"stated they did not consent\")\n",
        "T1_QUOTE_RE = re.compile(r\"[“”\\\"']\", re.I)\n",
        "T1_TIME_RE  = re.compile(r\"\\b(\\d{1,2}:\\d{2}\\s?(?:am|pm)?)\\b|\\b(\\d{1,2}\\s?(?:am|pm))\\b|\\b(\\d{4}-\\d{2}-\\d{2})\\b\", re.I)\n",
        "T1_PLACE_WORDS = {\n",
        "    \"street\",\"st.\",\"st\",\"avenue\",\"ave\",\"road\",\"rd\",\"boulevard\",\"blvd\",\"lane\",\"ln\",\n",
        "    \"drive\",\"dr\",\"park\",\"city\",\"county\",\"block\",\"intersection\",\"apt\",\"apartment\",\"unit\", \"lakeside\"\n",
        "}\n",
        "#neutral action patterns\n",
        "T1_ACTION_PHRASES = {\n",
        "    \"subject walked away\",\"victim stated they did not consent\",\"victim stated no\",\n",
        "    \"victim reported\",\"officer observed\",\"witness stated\",\"was located at\",\"arrived at\"\n",
        "}\n",
        "\n",
        "#tier 2 — subjective language potentially biasing against victim\n",
        "SKEPTICAL_QUALIFIERS = {\n",
        "    \"alleges\",\"alleged\",\"claims\",\"claimed\",\"supposedly\",\"states she felt\",\"states he felt\",\n",
        "    \"she claims\",\"he claims\"\n",
        "}\n",
        "DEMEANOR_PHRASES = {\n",
        "    \"appeared unusually calm\",\"did not cry\",\"seemed detached\",\"unemotional\",\"spoke matter-of-fact\"\n",
        "}\n",
        "VICTIM_CHARACTER = {\n",
        "    \"promiscuous\",\"bad reputation\",\"weak victim\",\"sex worker\",\"prostitute\",\"regular drug user\",\n",
        "    \"addict\",\"mental\"\n",
        "}\n",
        "INTOXICATION_FOCUS = {\"intoxicated\",\"heavily intoxicated\",\"under the influence\",\"inebriated\",\"drinking\"}\n",
        "TIME_GAP_PHRASES = {\"did not report until\",\"occurred two nights ago\",\"later said it was around\"}\n",
        "SELF_BLAME_PHRASES = {\"i shouldn't have gone\",\"i feel responsible\",\"i am responsible\",\"my fault\"}\n",
        "RELATIONSHIP_CONTEXT = {\"prior relationship\",\"knew each other\"}\n",
        "\n",
        "#tier 3 — language undermining MPC elements (force/threat, incapacitation, lack of consent)\n",
        "EUPHEMISMS = {\"intercourse occurred\",\"sexual relations\",\"had sex\",\"the incident\",\"sex happened\"}\n",
        "MINIMIZERS = {\"no trauma\",\"no fear\"}\n",
        "VAGUE_ALCOHOL = {\"both parties had been drinking\"}\n",
        "\n",
        "#legal element coverage\n",
        "FORCE_TERMS   = {\"grabbed\",\"pushed\",\"hit\",\"struck\",\"choked\",\"threatened\",\"forced\",\"held down\",\"pinned\",\"pushed her onto the bed\",\"pushed him onto the bed\"}\n",
        "CONSENT_TERMS = {\"without consent\",\"said no\",\"told him to stop\",\"told her to stop\",\"refused\",\"non-consensual\",\"rape\",\"sexual assault\",\"penetration without consent\"}\n",
        "INCAP_TERMS   = {\"unconscious\",\"slurred speech\",\"impaired\",\"vomiting\",\"memory loss\",\"blackout\",\"found unconscious\",\"could not stand\",\"substantially impaired\"}\n",
        "\n",
        "#non-consensual explicit terms (contrast with euphemisms)\n",
        "NONCONS_EXPLICIT = {\"rape\",\"sexual assault\",\"penetration without consent\",\"non-consensual\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lJb9O1V5XSl"
      },
      "source": [
        "Categories that came directly from the \"Linguistic Triggers\" Google doc. Phrases are pretty limited; I would either add more phrases to each category to avoid overfitting to our specific transcripts or explore a way for the code use these phrases as a base and be able to identify other phrases as these same categories on its own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIv1OEwTmV07"
      },
      "outputs": [],
      "source": [
        "#extracting features\n",
        "\n",
        "def _txt(x):\n",
        "    return \"\" if pd.isna(x) else str(x)\n",
        "\n",
        "#lowercase\n",
        "def _low(x):\n",
        "    return _txt(x).lower()\n",
        "\n",
        "#counts how many times the target words appear as substrings in the transcript\n",
        "def count_contains(text, terms):\n",
        "    t = _low(text)\n",
        "    return sum(1 for w in terms if w in t)\n",
        "\n",
        "def has_regex(text, regex):\n",
        "    return 1 if regex.search(_txt(text)) else 0\n",
        "\n",
        "def count_place_words(text, place_words=T1_PLACE_WORDS):\n",
        "    t = f\" {_low(text)} \"  #pad for simple word boundary\n",
        "    return sum(t.count(f\" {w} \") for w in place_words)\n",
        "\n",
        "def count_action_phrases(text, phrases=T1_ACTION_PHRASES):\n",
        "    t = _low(text)\n",
        "    return sum(1 for p in phrases if p in t)\n",
        "\n",
        "def sentiment_subjectivity(text):\n",
        "    tb = TextBlob(_txt(text))\n",
        "    return tb.sentiment.polarity, tb.sentiment.subjectivity\n",
        "\n",
        "def extract_features(report_text: str) -> pd.Series:\n",
        "    #Sentiment\n",
        "    pol, sub = sentiment_subjectivity(report_text)\n",
        "\n",
        "    #Tier 1 (objective) indicators\n",
        "    t1_quotes = has_regex(report_text, T1_QUOTE_RE)\n",
        "    t1_time = has_regex(report_text, T1_TIME_RE)\n",
        "    t1_place_hits = count_place_words(report_text)\n",
        "    t1_actions = count_action_phrases(report_text)\n",
        "\n",
        "    #Tier 2 (subjective/victim-biasing) indicators\n",
        "    t2_skeptic = count_contains(report_text, SKEPTICAL_QUALIFIERS)\n",
        "    t2_demeanor = count_contains(report_text, DEMEANOR_PHRASES)\n",
        "    t2_character = count_contains(report_text, VICTIM_CHARACTER)\n",
        "    t2_intox = count_contains(report_text, INTOXICATION_FOCUS)\n",
        "    t2_timegap = count_contains(report_text, TIME_GAP_PHRASES)\n",
        "    t2_selfblame = count_contains(report_text, SELF_BLAME_PHRASES)\n",
        "    t2_rel = count_contains(report_text, RELATIONSHIP_CONTEXT)\n",
        "\n",
        "    #Tier 3 (undermining MPC) indicators\n",
        "    t3_euph = count_contains(report_text, EUPHEMISMS)\n",
        "    t3_min = count_contains(report_text, MINIMIZERS)\n",
        "    t3_vaguealc = count_contains(report_text, VAGUE_ALCOHOL)\n",
        "\n",
        "    #Legal element coverage (presence counts)\n",
        "    legal_force = count_contains(report_text, FORCE_TERMS)\n",
        "    legal_consent = count_contains(report_text, CONSENT_TERMS)\n",
        "    legal_incap = count_contains(report_text, INCAP_TERMS)\n",
        "    legal_noncons = count_contains(report_text, NONCONS_EXPLICIT)\n",
        "\n",
        "    return pd.Series({\n",
        "        \"Polarity\": pol,\n",
        "        \"Subjectivity\": sub,\n",
        "\n",
        "        #Tier 1 signals (higher = more objective style)\n",
        "        \"T1_Quotes\": t1_quotes,\n",
        "        \"T1_TimeMention\": t1_time,\n",
        "        \"T1_PlaceWordHits\": t1_place_hits,\n",
        "        \"T1_ActionPhrases\": t1_actions,\n",
        "\n",
        "        #Tier 2 signals (subjective / victim-biasing)\n",
        "        \"T2_SkepticalQual\": t2_skeptic,\n",
        "        \"T2_Demeanor\": t2_demeanor,\n",
        "        \"T2_Character\": t2_character,\n",
        "        \"T2_IntoxFocus\": t2_intox,\n",
        "        \"T2_TimeGap\": t2_timegap,\n",
        "        \"T2_SelfBlame\": t2_selfblame,\n",
        "        \"T2_Relationship\": t2_rel,\n",
        "\n",
        "        #Tier 3 signals (undermining MPC elements)\n",
        "        \"T3_Euphemisms\": t3_euph,\n",
        "        \"T3_Minimizers\": t3_min,\n",
        "        \"T3_VagueAlcohol\": t3_vaguealc,\n",
        "\n",
        "        #Legal coverage (presence; *lower* may indicate omission/minimization)\n",
        "        \"Legal_Force\": legal_force,\n",
        "        \"Legal_Consent\": legal_consent,\n",
        "        \"Legal_Incapacitation\": legal_incap,\n",
        "        \"Legal_NonConsExplicit\": legal_noncons,\n",
        "    })\n",
        "if COL_TEXT not in reports.columns:\n",
        "    raise KeyError(f\"Column '{COL_TEXT}' not found in loaded batch files. Available: {list(reports.columns)}\")\n",
        "\n",
        "#Extract features for each report\n",
        "features = reports[COL_TEXT].apply(extract_features)\n",
        "reports_feats = pd.concat(\n",
        "    [reports[[COL_CASE, \"Tier\", COL_TEXT, \"_batch_num\"]], features],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Rows with features:\", len(reports_feats))\n",
        "reports_feats.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBdhjudy53gV"
      },
      "source": [
        "The above code counts the exact number of times our specific category phrases appear in the transcript. With this, you can see exactly how many occurences are in each T1, T2, T3 category. As you can see, even T1 can contain \"T3 features.\" These are just frequencies of hard-coded words; we can rework this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ENRbFl7mYOm"
      },
      "outputs": [],
      "source": [
        "#tier based\n",
        "\n",
        "agg_cols = [\n",
        "    \"Polarity\",\"Subjectivity\",\n",
        "    \"T1_Quotes\",\"T1_TimeMention\",\"T1_PlaceWordHits\",\"T1_ActionPhrases\",\n",
        "    \"T2_SkepticalQual\",\"T2_Demeanor\",\"T2_Character\",\"T2_IntoxFocus\",\"T2_TimeGap\",\"T2_SelfBlame\",\"T2_Relationship\",\n",
        "    \"T3_Euphemisms\",\"T3_Minimizers\",\"T3_VagueAlcohol\",\n",
        "    \"Legal_Force\",\"Legal_Consent\",\"Legal_Incapacitation\",\"Legal_NonConsExplicit\",\n",
        "]\n",
        "\n",
        "summary = reports_feats.groupby(\"Tier\")[agg_cols].mean().round(3).reset_index()\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34kloZ5n4TOa"
      },
      "source": [
        "Code above clears the Case ID, Incident Report, and batch numbers from the dataset. Subjectivity shows whether text is more factual or opinion based. 0 represents objective and 1 represents highly subjective text. Polarity shows if the text is positive, negative, or neutral. 0 is negative and 1 is positive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tier_summary_path = os.path.join(RESULTS_DIR, \"tier_summary_attempt1.csv\")\n",
        "reports_feats_path = os.path.join(RESULTS_DIR, \"reports_features_attempt1.csv\")\n",
        "\n",
        "summary.to_csv(tier_summary_path, index=False)\n",
        "reports_feats.to_csv(reports_feats_path, index=False)\n",
        "\n",
        "print(\"Saved Attempt 1 outputs:\")\n",
        "print(\" -\", tier_summary_path)\n",
        "print(\" -\", reports_feats_path)"
      ],
      "metadata": {
        "id": "CvoK0__nSuPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot: Mean Subjectivity by Tier\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(summary[\"Tier\"], summary[\"Subjectivity\"])\n",
        "plt.xlabel(\"Tier\")\n",
        "plt.ylabel(\"Mean Subjectivity\")\n",
        "plt.title(\"Mean Subjectivity by Tier\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(RESULTS_DIR, \"plot_mean_subjectivity_by_tier.png\")\n",
        "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"Saved plot:\", plot_path)"
      ],
      "metadata": {
        "id": "hibdeuXUjSFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot: Distribution of Subjectivity across all reports\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(reports_feats[\"Subjectivity\"], bins=20)\n",
        "plt.xlabel(\"Subjectivity\")\n",
        "plt.ylabel(\"Count of Reports\")\n",
        "plt.title(\"Distribution of Subjectivity Across All Reports\")\n",
        "plt.tight_layout()\n",
        "hist_path = os.path.join(RESULTS_DIR, \"hist_subjectivity_all_reports.png\")\n",
        "plt.savefig(hist_path, dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"Saved plot:\", hist_path)"
      ],
      "metadata": {
        "id": "x8OZR-3DjWCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keyword amplification (prompt vs report)\n",
        "\n",
        "NEUTRAL_OBJECTIVE = set(T1_ACTION_PHRASES)\n",
        "\n",
        "SUBJECTIVE_NEGATIVE = (\n",
        "    SKEPTICAL_QUALIFIERS\n",
        "    | DEMEANOR_PHRASES\n",
        "    | VICTIM_CHARACTER\n",
        "    | INTOXICATION_FOCUS\n",
        "    | TIME_GAP_PHRASES\n",
        "    | SELF_BLAME_PHRASES\n",
        "    | RELATIONSHIP_CONTEXT\n",
        ")\n",
        "\n",
        "LEGAL_HIGH_IMPACT = (\n",
        "    EUPHEMISMS\n",
        "    | MINIMIZERS\n",
        "    | VAGUE_ALCOHOL\n",
        ")\n",
        "\n",
        "TRIGGER_BUCKETS = {\n",
        "    \"NeutralObjective\": NEUTRAL_OBJECTIVE,\n",
        "    \"SubjectiveNegative\": SUBJECTIVE_NEGATIVE,\n",
        "    \"LegalHighImpact\":   LEGAL_HIGH_IMPACT,\n",
        "}\n",
        "\n",
        "def count_phrases(text, phrases):\n",
        "    t = \"\"\n",
        "    if not pd.isna(text):\n",
        "        t = str(text).lower()\n",
        "    total = 0\n",
        "    for p in phrases:\n",
        "        if not p:\n",
        "            continue\n",
        "        pattern = r\"\\b\" + re.escape(p.lower()) + r\"\\b\"\n",
        "        total += len(re.findall(pattern, t))\n",
        "    return total\n",
        "\n",
        "def compute_keyword_amplification(row):\n",
        "    prompt_text = row.get(COL_TRANSCRIPT, \"\")\n",
        "    report_text = row.get(COL_TEXT, \"\")\n",
        "\n",
        "    out = {}\n",
        "    for bucket_name, phrases in TRIGGER_BUCKETS.items():\n",
        "        prompt_count = count_phrases(prompt_text, phrases)\n",
        "        report_count = count_phrases(report_text, phrases)\n",
        "        delta = report_count - prompt_count\n",
        "\n",
        "        if prompt_count > 0:\n",
        "            ratio = report_count / prompt_count\n",
        "        else:\n",
        "            ratio = float(\"inf\") if report_count > 0 else 0.0\n",
        "\n",
        "        out[f\"{bucket_name}_PromptCount\"] = prompt_count\n",
        "        out[f\"{bucket_name}_ReportCount\"] = report_count\n",
        "        out[f\"{bucket_name}_Delta\"] = delta\n",
        "        out[f\"{bucket_name}_Ratio\"] = ratio\n",
        "\n",
        "    return pd.Series(out)"
      ],
      "metadata": {
        "id": "OkUqxIjqjdcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reports_with_sentiment = reports + Attempt 1 features\n",
        "base_cols = [COL_CASE, COL_TRANSCRIPT, \"Tier\", COL_TEXT, \"_batch_num\"]\n",
        "base_cols_filtered = [c for c in base_cols if c in reports.columns]\n",
        "reports_with_sentiment = pd.concat(\n",
        "    [reports[base_cols_filtered], features],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Rows with sentiment/features:\", len(reports_with_sentiment))\n",
        "\n",
        "keyword_feats = reports_with_sentiment.apply(compute_keyword_amplification, axis=1)\n",
        "reports_with_bias = pd.concat([reports_with_sentiment, keyword_feats], axis=1)\n",
        "\n",
        "print(\"Rows with bias/keyword feats:\", len(reports_with_bias))\n",
        "reports_with_bias.head(3)"
      ],
      "metadata": {
        "id": "UGn6fYPgjj-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean infinities and save keyword amplification table\n",
        "\n",
        "reports_with_bias.replace([float(\"inf\"), float(\"-inf\")], pd.NA, inplace=True)\n",
        "\n",
        "rwb_path = os.path.join(RESULTS_DIR, \"reports_with_bias_keyword_amplification.csv\")\n",
        "reports_with_bias.to_csv(rwb_path, index=False)\n",
        "print(\"Saved bias / keyword amplification table to:\", rwb_path)"
      ],
      "metadata": {
        "id": "hPBt9H1rjmvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot: mean SubjectiveNegative_Ratio by Tier\n",
        "\n",
        "if \"SubjectiveNegative_Ratio\" in reports_with_bias.columns:\n",
        "    tier_keyword = (\n",
        "        reports_with_bias\n",
        "        .groupby(\"Tier\")[\"SubjectiveNegative_Ratio\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    plt.figure()\n",
        "    plt.bar(tier_keyword[\"Tier\"], tier_keyword[\"SubjectiveNegative_Ratio\"])\n",
        "    plt.xlabel(\"Tier\")\n",
        "    plt.ylabel(\"Mean SubjectiveNegative Ratio (Report / Transcript)\")\n",
        "    plt.title(\"Keyword Amplification: Subjective/Negative Language by Tier\")\n",
        "    plt.xticks(rotation=30, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plot_kw_path = os.path.join(RESULTS_DIR, \"plot_subjective_negative_ratio_by_tier.png\")\n",
        "    plt.savefig(plot_kw_path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(\"Saved plot:\", plot_kw_path)\n",
        "else:\n",
        "    print(\"SubjectiveNegative_Ratio column not found; did keyword amplification run?\")\n"
      ],
      "metadata": {
        "id": "M0V2a4RUjrza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXe2mtjo8Oua"
      },
      "source": [
        "### Links\n",
        "https://www.geeksforgeeks.org/python/python-textblob-sentiment-method/\n",
        "\n",
        "https://realpython.com/python-keras-text-classification/\n",
        "\n",
        "https://huggingface.co/tasks/zero-shot-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVpXMFu3vCNY"
      },
      "source": [
        "# Bias Scale (Attempt 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fnS0iKvvXve"
      },
      "source": [
        "For this second attempt, we are focusing on specific labeling and legal coverage. Trying to expand further than just subjective/objective or positive/negative. We will ask the model specific questions relating to\n",
        "\n",
        "1. The victim (Stance labels)\n",
        "    * Is this sentence objective, skeptical of the victim, minimizing their experience, etc.\n",
        "2. The context of the report (Legal labels)\n",
        "    * Does this sentence describe force, lack of consent, incapacitation, etc.\n",
        "\n",
        "We will later tie the three tiers into our findings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SyzwrqP5xTbg"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers torch sentencepiece nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04QtAChnvLyo"
      },
      "outputs": [],
      "source": [
        "#basic setup\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ke9nVeDbxbho"
      },
      "outputs": [],
      "source": [
        "#loading in our zero-shot classifier\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model = \"facebook/bart-large-mnli\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJRBkluMzDs8"
      },
      "source": [
        "Zero shot classification essentially allows us to train a model on a set of example labels, and then have it classify unseens examples. This way, we don't have to hard code specific phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA59eKBoxzQa"
      },
      "outputs": [],
      "source": [
        "#creating our stance and legal labels/descriptions\n",
        "#multiple stance labels\n",
        "STANCE_OBJECTIVE = [\n",
        "    \"Objective and factual description of events without interpretation or judgment\",\n",
        "    \"Direct quoting of the victim or suspect without added commentary\",\n",
        "    \"Factual description of injuries, evidence, time, and location\",\n",
        "]\n",
        "\n",
        "STANCE_VICTIM_SUPPORTIVE = [\n",
        "    \"Language that clearly supports the victim and recognizes lack of consent\",\n",
        "]\n",
        "\n",
        "STANCE_VICTIM_UNDERMINING = [\n",
        "    \"Language that implies victim culpability or risky behavior, such as intoxication, clothing, or prior relationship\",\n",
        "    \"Language that alleges victim character flaws such as promiscuity, mental instability, drug use, or reputation\",\n",
        "    \"Language that questions the victim’s credibility through descriptions of demeanor such as being calm, detached, or unemotional\",\n",
        "    \"Language that emphasizes delays in reporting or inconsistencies in the victim’s timeline\",\n",
        "    \"Language that uses skeptical qualifiers such as alleges, claims, supposedly, or quotation marks around victim statements\",\n",
        "    \"Language that includes victim self-blame statements without contextualizing trauma responses\",\n",
        "    \"Language in which the officer expresses subjective doubts about the victim’s credibility or calls the victim weak\",\n",
        "]\n",
        "\n",
        "STANCE_LEGAL_OBSCURING = [\n",
        "    \"Language that obscures or minimizes the use of force or threats\",\n",
        "    \"Language that obscures or downplays the victim’s incapacitation\",\n",
        "    \"Language that obscures explicit lack of consent or ignores non-consent signals\",\n",
        "    \"Language that uses euphemisms or neutral terms to describe non-consensual sexual acts\",\n",
        "]\n",
        "\n",
        "#legal labels\n",
        "LEGAL_GOOD_COVERAGE = [\n",
        "    \"Describes specific acts of force or threats used against the victim\",\n",
        "    \"Describes the victim being incapacitated or substantially impaired\",\n",
        "    \"Describes explicit lack of consent or resistance from the victim\",\n",
        "]\n",
        "\n",
        "LEGAL_EUPHEMISM_OR_OMISSION = [\n",
        "    \"Uses euphemistic or vague language instead of naming non-consensual sexual acts\",\n",
        "    \"Omits or minimizes legally relevant details showing force, threat, incapacitation, or non-consent\",\n",
        "]\n",
        "\n",
        "STANCE_LABELS = (\n",
        "    STANCE_OBJECTIVE\n",
        "    + STANCE_VICTIM_SUPPORTIVE\n",
        "    + STANCE_VICTIM_UNDERMINING\n",
        "    + STANCE_LEGAL_OBSCURING\n",
        ")\n",
        "\n",
        "LEGAL_LABELS = (\n",
        "    LEGAL_GOOD_COVERAGE\n",
        "    + LEGAL_EUPHEMISM_OR_OMISSION\n",
        ")\n",
        "\n",
        "def _avg(scores_dict, keys):\n",
        "    vals = [scores_dict[k] for k in keys if k in scores_dict]\n",
        "    return sum(vals) / len(vals) if vals else 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PbihjBcI6VP"
      },
      "source": [
        "Here we are using zero shot classification on a single sentence. We'll analyze and score one sentence on the specific biases created above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cg6wee_yXbe"
      },
      "outputs": [],
      "source": [
        "#trying zero shot classification on a single sentence\n",
        "def classify_sentence_zero_shot(sentence: str):\n",
        "    #stance / bias\n",
        "    stance = classifier(\n",
        "        sentence,\n",
        "        candidate_labels=STANCE_LABELS,\n",
        "        multi_label=True\n",
        "    )\n",
        "    stance_scores = dict(zip(stance[\"labels\"], stance[\"scores\"]))\n",
        "\n",
        "    #legal coverage\n",
        "    legal = classifier(\n",
        "        sentence,\n",
        "        candidate_labels=LEGAL_LABELS,\n",
        "        multi_label=True\n",
        "    )\n",
        "    legal_scores = dict(zip(legal[\"labels\"], legal[\"scores\"]))\n",
        "\n",
        "    return {\n",
        "        \"stance_scores\": stance_scores,\n",
        "        \"legal_scores\": legal_scores,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGa4nVwb0Fbh"
      },
      "source": [
        "Since our narratives/reports are multiple sentences, we can either\n",
        "1. We can split the narrative by sentences and analyze each\n",
        "2. Analyze the whole narrative at once\n",
        "\n",
        "We will try both and determine how each can be used."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bias_score_from_zero_shot(result):\n",
        "    stance = result[\"stance_scores\"]\n",
        "    legal  = result[\"legal_scores\"]\n",
        "\n",
        "    #group means\n",
        "    obj = _avg(stance, STANCE_OBJECTIVE)\n",
        "    support = _avg(stance, STANCE_VICTIM_SUPPORTIVE)\n",
        "    underm = _avg(stance, STANCE_VICTIM_UNDERMINING)\n",
        "    obsc_st = _avg(stance, STANCE_LEGAL_OBSCURING)\n",
        "\n",
        "    legal_good = _avg(legal, LEGAL_GOOD_COVERAGE)\n",
        "    legal_euph = _avg(legal, LEGAL_EUPHEMISM_OR_OMISSION)\n",
        "\n",
        "    #scoring logic\n",
        "    #higher = more biased against victim / more legal obscuring\n",
        "    raw = 0.0\n",
        "\n",
        "    #victim-undermining narrative and legal-obscuring narrative\n",
        "    raw += 2.0 * underm\n",
        "    raw += 1.5 * obsc_st\n",
        "\n",
        "    #euphemistic / omission on legal axis\n",
        "    raw += 1.5 * legal_euph\n",
        "\n",
        "    #subtract supportive / objective / good legal coverage\n",
        "    raw -= 1.5 * support\n",
        "    raw -= 0.5 * obj\n",
        "    raw -= 0.5 * legal_good\n",
        "\n",
        "    #clip to a nice range [0, 1] then to [0, 10] if you want a 0–10 scale\n",
        "    #shift so that negative raw becomes 0\n",
        "    shifted = max(raw, 0.0)\n",
        "    #squish big numbers\n",
        "    normalized = min(shifted, 3.0) / 3.0    # 0–1\n",
        "    score_0_10 = normalized * 10.0          # 0–10\n",
        "\n",
        "    return {\n",
        "        \"raw\": raw,\n",
        "        \"normalized_0_1\": normalized,\n",
        "        \"bias_score_0_10\": score_0_10,\n",
        "        \"components\": {\n",
        "            \"objective\": obj,\n",
        "            \"victim_supportive\": support,\n",
        "            \"victim_undermining\": underm,\n",
        "            \"stance_legal_obscuring\": obsc_st,\n",
        "            \"legal_good_coverage\": legal_good,\n",
        "            \"legal_euphemism_or_omission\": legal_euph,\n",
        "        }\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Z0yVcisNlERx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute bias scale scores for all reports (attempt 2)\n",
        "def compute_bias_for_report(text: str) -> pd.Series:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return pd.Series({\n",
        "            \"BiasRaw\": 0.0,\n",
        "            \"BiasScore_0_10\": 0.0,\n",
        "            \"Comp_objective\": 0.0,\n",
        "            \"Comp_victim_supportive\": 0.0,\n",
        "            \"Comp_victim_undermining\": 0.0,\n",
        "            \"Comp_stance_legal_obscuring\": 0.0,\n",
        "            \"Comp_legal_good_coverage\": 0.0,\n",
        "            \"Comp_legal_euphemism_or_omission\": 0.0,\n",
        "        })\n",
        "\n",
        "    result = classify_sentence_zero_shot(text)\n",
        "    score_dict = compute_bias_score_from_zero_shot(result)\n",
        "\n",
        "    out = {\n",
        "        \"BiasRaw\": score_dict[\"raw\"],\n",
        "        \"BiasScore_0_10\": score_dict[\"bias_score_0_10\"],\n",
        "    }\n",
        "    for k, v in score_dict[\"components\"].items():\n",
        "        out[f\"Comp_{k}\"] = v\n",
        "    return pd.Series(out)\n",
        "\n",
        "print(\"Computing bias scores for all reports (this may take a while)...\")\n",
        "\n",
        "bias_scores = reports_with_bias[COL_TEXT].apply(compute_bias_for_report)\n",
        "reports_with_full_bias = pd.concat([reports_with_bias, bias_scores], axis=1)\n",
        "\n",
        "bias_csv_path = os.path.join(RESULTS_DIR, \"reports_with_bias_attempt2.csv\")\n",
        "reports_with_full_bias.to_csv(bias_csv_path, index=False)\n",
        "print(\"Saved Attempt 2 bias scores to:\", bias_csv_path)"
      ],
      "metadata": {
        "id": "Z3iFw6PNkVyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mean bias score by tier\n",
        "bias_by_tier = (\n",
        "    reports_with_full_bias\n",
        "    .groupby(\"Tier\")[\"BiasScore_0_10\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .sort_values(\"Tier\")\n",
        ")\n",
        "bias_by_tier_path = os.path.join(RESULTS_DIR, \"bias_score_by_tier_attempt2.csv\")\n",
        "bias_by_tier.to_csv(bias_by_tier_path, index=False)\n",
        "print(\"Saved mean bias score by tier to:\", bias_by_tier_path)\n",
        "bias_by_tier\n"
      ],
      "metadata": {
        "id": "9-sPY5cjkhMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot: Bias Score (0–10) by Tier\n",
        "plt.figure()\n",
        "plt.bar(bias_by_tier[\"Tier\"], bias_by_tier[\"BiasScore_0_10\"])\n",
        "plt.xlabel(\"Tier\")\n",
        "plt.ylabel(\"Mean Bias Score (0–10)\")\n",
        "plt.title(\"Attempt 2 Bias Scale Score by Tier\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_bias_path = os.path.join(RESULTS_DIR, \"plot_bias_score_by_tier_attempt2.png\")\n",
        "plt.savefig(plot_bias_path, dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"Saved plot:\", plot_bias_path)"
      ],
      "metadata": {
        "id": "x_3879Fmkkv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRT--oeRJbJN"
      },
      "source": [
        "Here we test on an example to see the output scores on the sentence. The code below analyzes a real sentence from our generated reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBuQclW0Rph"
      },
      "outputs": [],
      "source": [
        "narr = \"\"\"\n",
        "Keira exhibited no visible physical injuries but reported experiencing anxiety and shaking.\n",
        "\"\"\"\n",
        "analysis = classify_sentence_zero_shot(narr)\n",
        "final = compute_bias_score_from_zero_shot(analysis)\n",
        "final[\"bias_score_0_10\"], final[\"components\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeERuD-eLBUo"
      },
      "source": [
        "If we analyze an entire narrative sentence by sentence, the process would take a long time to complete for each narrative. Because of this, I think we could use sentence analysis is we want to look at a specific narrative in more detail. For example, after we determine which reports are \"tier 3,\" we can analyze a specific tier 3 prompt and see what sentences scored the highest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8vdaip6NDHP"
      },
      "source": [
        "The narrative below was the result of a Tier 3 transcript (Case ID: 141). Next, we experiment with analyzing the whole narrative at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMC-0zMU3ZbW",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#real tier 3 narrative (pasting it word for word for testing purposes; we'll automate this to pull out the summary from the reports later)\n",
        "narr = \"\"\"\n",
        "CASE_ID: 141\n",
        "\n",
        "On November 21, 2025, at approximately 22:02 hours, I, Officer Ebner (#2339), responded to a report of a sexual assault at a local restaurant, specifically in the staff locker corridor. The weather was cloudy with a temperature of 69°F. Upon arrival, I made contact with the reporting party, identified as Camila, who stated that after the restaurant had closed, a male coworker touched her chest without consent. Camila reported that she immediately said “No” and left the area.\n",
        "\n",
        "During the interview, I noted that the kitchen corridors in the establishment are typically crowded during operating hours, and brief physical contact can sometimes be incidental due to the confined space. I asked Camila if there was any physical restraint or injury resulting from the incident. She confirmed that there were no marks or injuries and that she did not shout or call out during the encounter.\n",
        "\n",
        "I further inquired about the context and whether there might have been any misunderstanding or lack of awareness on the part of the alleged offender. Camila mentioned that coworkers often tease each other, suggesting the possibility that the male employee may not have been fully aware of the inappropriateness of his actions.\n",
        "\n",
        "There was no CCTV footage available from the corridor where the incident occurred, nor were there any witnesses present to corroborate the report. Camila did not have any photographic evidence, and no Sexual Assault Nurse Examiner (SANE) or other medical resources were utilized or requested.\n",
        "\n",
        "Based on the information gathered, there was no indication of compulsion, physical injury, or incapacitation. The intent behind the contact was ambiguous, and the incident appeared to be more of an administrative concern rather than a criminal matter. I advised Camila of her options and the procedures for reporting any future incidents. The case was documented accordingly.\n",
        "\n",
        "At 22:55 hours, I cleared the scene with no further action taken at that time. Officer Fox (#2577) was present as secondary during the response and assisted as needed.\n",
        "\n",
        "Report completed by Officer Ebner (#2339).\n",
        "\"\"\"\n",
        "analysis = classify_sentence_zero_shot(narr)\n",
        "final = compute_bias_score_from_zero_shot(analysis)\n",
        "final[\"bias_score_0_10\"], final[\"components\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#real tier 3 narrative (pasting it word for word for testing purposes; we'll automate this to pull out the summary from the reports later)\n",
        "\n",
        "narr =\"\"\" CASE_ID: 142\n",
        "\n",
        "On November 22, 2025, at approximately 21:04 hours, I, Officer Doyle (#3198), along with Officer Kravitz (#2902), responded to a call regarding a reported sexual assault at the 700 Grand Hotel, Room 605. The weather was cloudy with a temperature of 70°F. Upon arrival, we made contact with the complainant, identified as Lena, who provided an account of the incident.\n",
        "\n",
        "Lena stated that after consuming drinks at the hotel bar, an individual placed his hand inside her dress without her consent. She clearly told him \"No\" and indicated her desire for him to leave by pointing toward the door. According to her statement, the individual attempted the same action once more despite her refusal.\n",
        "\n",
        "During the initial assessment, I considered the context of the interaction, noting that the complainant and the subject had been flirting at the bar prior to the incident. I explained that while flirtation can suggest mutual interest, a single verbal refusal, such as \"No,\" might sometimes be interpreted as ambiguous or mixed messaging in social situations. However, I emphasized that for an incident to meet the threshold of sexual assault under the law, there must be evidence of force, restraint, or injury.\n",
        "\n",
        "I inquired about any physical injuries or signs of struggle, but Lena reported none. There were no visible injuries or indications of physical coercion. Additionally, no medical examination by a Sexual Assault Nurse Examiner (SANE) was conducted or requested at the time of our investigation. I also noted that keycard logs, hallway surveillance footage, and potential witnesses were not reviewed or considered during the initial report.\n",
        "\n",
        "Based on the information gathered, including the absence of force, restraint, injury, or corroborating evidence, I concluded that the incident did not meet the prosecutorial standards for sexual assault. The complainant’s description suggested unwanted contact, but without additional elements such as physical coercion or injury, the case was classified as non-prosecutable.\n",
        "\n",
        "Throughout the interaction, I maintained a professional and empathetic demeanor, ensuring Lena was informed of her options and resources, although no immediate referrals to support services were made. The entire response and investigation at the scene lasted until approximately 21:57 hours, at which point we cleared the location.\n",
        "\n",
        "In summary, the incident involved unwanted physical contact following a social interaction characterized by mutual flirtation. The complainant’s refusal was noted, but the lack of force, injury, or corroborative evidence led to the determination that the case did not meet the criteria for further legal action.\n",
        "\n",
        "Report completed by Officer Doyle (#3198).\n",
        "\"\"\"\n",
        "\n",
        "analysis = classify_sentence_zero_shot(narr)\n",
        "final = compute_bias_score_from_zero_shot(analysis)\n",
        "final[\"bias_score_0_10\"], final[\"components\"]\n"
      ],
      "metadata": {
        "id": "lWq9z11pGzca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#real tier 3 narrative (pasting it word for word for testing purposes; we'll automate this to pull out the summary from the reports later)\n",
        "\n",
        "narr =\"\"\" CASE_ID: 143\n",
        "\n",
        "On November 22, 2025, at approximately 07:06 hours, I, Officer Li (#2631), along with Officer Gomez (#2740), responded to a call regarding a sexual assault report at the Riverwalk area, specifically under the bridge at mile marker 1.8. The weather was breezy with a temperature of 75°F. Our time on scene was from 07:06:58 to 08:00:21.\n",
        "\n",
        "Upon arrival, I made contact with the reporting party, who identified herself as June. June stated that while she was near the railing, an unidentified male touched her in a manner she found inappropriate. She clearly recalled saying, “No, stop,” and then stepping away from the individual. June’s account suggested that the contact was unwanted, but she did not indicate any physical restraint or injury resulting from the encounter.\n",
        "\n",
        "During the interview, I explained the legal standards relevant to her report. I informed June that unless the suspect had restrained her or caused physical harm, the incident might not meet the statutory requirements for a sexual assault under current law. I further clarified that the element of mens rea—meaning the suspect’s knowledge or intent to commit a crime—was unsettled in this case, especially since the suspect did not acknowledge her verbal refusal.\n",
        "\n",
        "No physical evidence was collected at the scene, and June declined or was not offered a Sexual Assault Nurse Examiner (SANE) examination or other medical resources. Given the absence of compulsion, injury, or clear acknowledgment of non-consent by the suspect, I summarized the situation as ambiguous with respect to criminal intent and compulsion.\n",
        "\n",
        "Throughout the investigation, I maintained a professional and empathetic demeanor, ensuring June felt heard and supported despite the limitations of the case. I advised her of her options should she wish to pursue further action or require additional support services.\n",
        "\n",
        "After completing the initial report and ensuring the scene was secure, we cleared the location at 08:00:21. The case remains documented under CAD# 25-242200, with no further immediate investigative steps taken due to the lack of evidence and statutory criteria.\n",
        "\n",
        "This concludes my report on the incident at Riverwalk under the bridge on November 22, 2025.\n",
        "\"\"\"\n",
        "\n",
        "analysis = classify_sentence_zero_shot(narr)\n",
        "final = compute_bias_score_from_zero_shot(analysis)\n",
        "final[\"bias_score_0_10\"], final[\"components\"]\n"
      ],
      "metadata": {
        "id": "l3vz5mFmIAFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now experimenting with tier 1 narratives."
      ],
      "metadata": {
        "id": "YmEvHfrWKuKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "narr = \"\"\"CASE_ID: 1\n",
        "\n",
        "On April 6, 2025, at approximately 02:06 hours, I, Police Officer Patel (#5123), along with Secondary Officer Nguyen (#4419), responded to a reported sexual assault at the rear parking lot of 1190 N. Cedar Ave., City of Foxridge. The weather was light rain with a temperature of 47°F. Upon arrival, we observed a gray four-door sedan parked with the rear passenger door slightly ajar. A female adult, later identified as Ari [last name withheld], was seated on the curb near the vehicle, wrapped in a blanket provided by her friend, Kira [last name withheld], who stood nearby.\n",
        "\n",
        "I introduced myself and informed Ari that we were recording on our body cameras and asked for her consent to speak on scene, which she granted. Ari appeared alert and oriented, with steady breathing and no visible external injuries on her face or forearms. She reported soreness in her inner thighs. Kira confirmed she had called 911 at 02:01 after receiving a distressed phone call from Ari at 01:58.\n",
        "\n",
        "Ari stated that the suspect, identified as Luis, a coworker, had put her in the backseat of the vehicle against her will. She described how Luis had offered to walk her to her car after leaving a bar around 01:35. Upon arrival at the parking lot, Luis asked to sit in the car, but Ari expressed her desire to leave. Luis then opened the back door and nudged her inside despite her repeated verbal refusals, saying “stop” and “no.” Ari attempted to push him away, and she said, “He held my wrists.” She described Luis as a Hispanic male in his mid-30s, approximately 5’10” with a medium build, wearing a navy jacket, jeans, and a black cap. Luis was last seen leaving the lot in a dark SUV around 01:55.\n",
        "\n",
        "We requested EMS for a medical evaluation and a Sexual Assault Nurse Examiner (SANE) exam at the hospital. Ari consented to officers photographing the interior of the vehicle prior to EMS arrival. Upon inspection, the gray sedan’s back seat fabric was torn approximately four inches at the center seam, and tissue paper was found on the floorboard. A condom wrapper was located near the rear passenger footwell. The vehicle’s license plate was partially obscured by mud, and a temporary paper permit was visible inside the rear window. The permit was later confirmed to be tied to dealership inventory and not registered to Ari.\n",
        "\n",
        "I canvassed the area and spoke with the night attendant at a nearby laundromat, who agreed to request CCTV footage covering the time frame from 01:30 to 02:10. The footage could potentially capture Luis’s departure in the dark SUV.\n",
        "\n",
        "At 02:11, EMS arrived, and the lead paramedic, Renee, explained the medical and forensic options to Ari, who elected to proceed with the SANE exam. Ari consented to leave the vehicle in place and to the release of the sexual assault kit to law enforcement after the exam.\n",
        "\n",
        "At the hospital, the sexual assault kit and clothing items (black leggings, gray hoodie, and underwear) were sealed and received by Officer Nguyen at 02:47. The evidence was logged into the CAD and evidence module accordingly. An advocate arrived at 02:58 to provide support to Ari.\n",
        "\n",
        "We cleared the hospital at 03:03, concluding our involvement. The victim’s identity was verified, direct quotes were documented, and photographic evidence of the vehicle interior was collected with consent. No physical evidence was collected on scene due to weather conditions and pending the SANE exam. Follow-up actions include reviewing CCTV footage and continuing the investigation into the suspect Luis.\n",
        "\n",
        "End of report.\n",
        "\"\"\"\n",
        "\n",
        "analysis = classify_sentence_zero_shot(narr)\n",
        "final = compute_bias_score_from_zero_shot(analysis)\n",
        "final[\"bias_score_0_10\"], final[\"components\"]"
      ],
      "metadata": {
        "id": "4ME-eqnZKtZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdfberYW0XTu"
      },
      "source": [
        "Now, we will incorporate our original tiers. Bias level?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading all reports in consistent way\n",
        "\n",
        "BATCH_GLOB_ALL = os.path.join(BATCH_DIR, \"incident_reports_batch_*.xlsx\")\n",
        "COL_CASE = \"CASE_ID\"\n",
        "COL_TEXT = \"Incident_Report\"\n",
        "COL_TRANSCRIPT = \"Transcript\"\n",
        "\n",
        "def load_all_reports(pattern=BATCH_GLOB_ALL):\n",
        "    files = sorted(glob.glob(pattern))\n",
        "    if not files:\n",
        "        raise RuntimeError(f\"No report batch files found matching {pattern}\")\n",
        "    frames = []\n",
        "    for path in files:\n",
        "        m = re.search(r\"batch_(\\d+)\\.xlsx$\", path)\n",
        "        batch_num = int(m.group(1)) if m else None\n",
        "        dfb = pd.read_excel(path)\n",
        "        dfb[\"_batch_num\"] = batch_num\n",
        "        frames.append(dfb)\n",
        "\n",
        "    reports = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    # ensure Tier exists (if not already)\n",
        "    if \"Tier\" not in reports.columns:\n",
        "        BATCH_TIER_MAP = {\n",
        "            1: \"Tier1_Neutral\",\n",
        "            2: \"Tier2_Subjective\",\n",
        "            3: \"Tier3_LegalUndermining\",\n",
        "        }\n",
        "        reports[\"Tier\"] = reports[\"_batch_num\"].map(BATCH_TIER_MAP).fillna(\"Unknown\")\n",
        "\n",
        "    return reports\n",
        "\n",
        "reports = load_all_reports()\n",
        "print(\"Loaded\", len(reports), \"reports\")\n",
        "reports.head()"
      ],
      "metadata": {
        "id": "mxZ1xzVByqU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentiment Analysis (per-report features using extract_features)\n",
        "features = reports[COL_TEXT].apply(extract_features)\n",
        "base_cols = [COL_CASE, COL_TRANSCRIPT, \"Tier\", COL_TEXT, \"_batch_num\"]\n",
        "base_cols_filtered = [c for c in base_cols if c in reports.columns]\n",
        "\n",
        "reports_with_sentiment = pd.concat([reports[base_cols_filtered], features], axis=1)\n",
        "\n",
        "print(\"Rows with features:\", len(reports_with_sentiment))\n",
        "reports_with_sentiment.head()"
      ],
      "metadata": {
        "id": "jpPxJLb10NPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save this table\n",
        "rws_path = os.path.join(RESULTS_DIR, \"reports_with_sentiment_attempt1.csv\")\n",
        "reports_with_sentiment.to_csv(rws_path, index=False)\n",
        "print(\"Saved per-report sentiment/features to:\", rws_path)"
      ],
      "metadata": {
        "id": "4sLlIBBfTYQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword Amplification\n",
        "\n",
        "NEUTRAL_OBJECTIVE = set(T1_ACTION_PHRASES)\n",
        "\n",
        "SUBJECTIVE_NEGATIVE = (\n",
        "    SKEPTICAL_QUALIFIERS\n",
        "    | DEMEANOR_PHRASES\n",
        "    | VICTIM_CHARACTER\n",
        "    | INTOXICATION_FOCUS\n",
        "    | TIME_GAP_PHRASES\n",
        "    | SELF_BLAME_PHRASES\n",
        "    | RELATIONSHIP_CONTEXT\n",
        ")\n",
        "\n",
        "LEGAL_HIGH_IMPACT = (\n",
        "    EUPHEMISMS\n",
        "    | MINIMIZERS\n",
        "    | VAGUE_ALCOHOL\n",
        ")\n",
        "\n",
        "TRIGGER_BUCKETS = {\n",
        "    \"NeutralObjective\": NEUTRAL_OBJECTIVE,\n",
        "    \"SubjectiveNegative\": SUBJECTIVE_NEGATIVE,\n",
        "    \"LegalHighImpact\":   LEGAL_HIGH_IMPACT,\n",
        "}\n",
        "\n",
        "def count_phrases(text, phrases):\n",
        "    t = \"\"\n",
        "    if not pd.isna(text):\n",
        "        t = str(text).lower()\n",
        "\n",
        "    total = 0\n",
        "    for p in phrases:\n",
        "        if not p:\n",
        "            continue\n",
        "        pattern = r\"\\b\" + re.escape(p.lower()) + r\"\\b\"\n",
        "        total += len(re.findall(pattern, t))\n",
        "    return total\n",
        "\n",
        "def compute_keyword_amplification(row):\n",
        "    prompt_text = row.get(COL_TRANSCRIPT, \"\")\n",
        "    report_text = row.get(COL_TEXT, \"\")\n",
        "\n",
        "    out = {}\n",
        "    for bucket_name, phrases in TRIGGER_BUCKETS.items():\n",
        "        prompt_count = count_phrases(prompt_text, phrases)\n",
        "        report_count = count_phrases(report_text, phrases)\n",
        "        delta = report_count - prompt_count\n",
        "\n",
        "        if prompt_count > 0:\n",
        "            ratio = report_count / prompt_count\n",
        "        else:\n",
        "            ratio = float(\"inf\") if report_count > 0 else 0.0\n",
        "\n",
        "        out[f\"{bucket_name}_PromptCount\"] = prompt_count\n",
        "        out[f\"{bucket_name}_ReportCount\"] = report_count\n",
        "        out[f\"{bucket_name}_Delta\"] = delta\n",
        "        out[f\"{bucket_name}_Ratio\"] = ratio\n",
        "\n",
        "    return pd.Series(out)\n",
        "\n",
        "keyword_feats = reports_with_sentiment.apply(compute_keyword_amplification, axis=1)\n",
        "reports_with_bias = pd.concat([reports_with_sentiment, keyword_feats], axis=1)\n",
        "\n",
        "print(\"Rows with bias features:\", len(reports_with_bias))\n",
        "reports_with_bias.head()"
      ],
      "metadata": {
        "id": "_MvpCWBq0y_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save bias / keyword amplification table\n",
        "rwb_path = os.path.join(RESULTS_DIR, \"reports_with_bias_keyword_amplification.csv\")\n",
        "reports_with_bias.to_csv(rwb_path, index=False)\n",
        "print(\"Saved bias / keyword amplification table to:\", rwb_path)"
      ],
      "metadata": {
        "id": "SMaqqIbkTfX-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TB-xwRRp_KWN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}